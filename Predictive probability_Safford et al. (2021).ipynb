{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona \n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define classes for node and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    def __init__(self, coor):\n",
    "        self.coor = coor # (long, lat)\n",
    "        self.sons = set() # downstream\n",
    "        self.father = set() # upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class waste_water_processor:\n",
    "    \n",
    "    def __init__(self, base_dir):\n",
    "        \n",
    "        self.base = base_dir #where to read saved files\n",
    "        if not self.base.endswith('/'):\n",
    "            self.base += '/'\n",
    "        self.init() #empty data structures\n",
    "        \n",
    "    def init(self):\n",
    "        self.coor_to_name = defaultdict(list)\n",
    "        self.all_nodes = {}\n",
    "        self.name_to_coor = {}\n",
    "        self.build_graph()\n",
    "    \n",
    "    def build_graph(self):\n",
    "        #load saved files to build the whole graph\n",
    "        data = pd.read_csv(self.base + 'connections.csv', dtype={'cbg':str, 'cb':str})\n",
    "        data['name'] = data['name'].apply(eval)\n",
    "        data['down_stream'] = data['down_stream'].apply(eval)\n",
    "        data['up_stream'] = data['up_stream'].apply(eval)\n",
    "        \n",
    "        #initialize basic nodes\n",
    "        print(\"Initializing Manholes\")\n",
    "        for row in tqdm.tqdm(range(data.shape[0])):\n",
    "            long = data.loc[row, 'long']\n",
    "            lat = data.loc[row, 'lat']\n",
    "            names = data.loc[row, 'name']\n",
    "            cbg = data.loc[row, 'cbg']\n",
    "            cb = data.loc[row, 'cb']\n",
    "            coor = (long, lat)\n",
    "            self.all_nodes[coor] = Node(coor)\n",
    "            self.coor_to_name[coor] = names\n",
    "            for name in names:\n",
    "                self.name_to_coor[name] = coor\n",
    "            self.all_nodes[coor].cb = cb\n",
    "            self.all_nodes[coor].cbg = cbg\n",
    "                \n",
    "        #connect upstreams and downstreams\n",
    "        print(\"Connecting Manholes\")\n",
    "        for row in tqdm.tqdm(range(data.shape[0])):\n",
    "            long = data.loc[row, 'long']\n",
    "            lat = data.loc[row, 'lat']\n",
    "            coor = (long, lat)\n",
    "            \n",
    "            downstream = data.loc[row, 'down_stream']\n",
    "            for next_coor in downstream:\n",
    "                next_coor = self.locate_coor(next_coor)\n",
    "                self.all_nodes[coor].sons |= set([next_coor])\n",
    "                \n",
    "            upstream = data.loc[row, 'up_stream']\n",
    "            for prev_coor in upstream:\n",
    "                prev_coor = self.locate_coor(prev_coor)\n",
    "                self.all_nodes[coor].father |= set([prev_coor])\n",
    "                \n",
    "        #check how many manholes are there in each census block\n",
    "        self.cb_counter = defaultdict(int)\n",
    "        for coor, node in self.all_nodes.items():\n",
    "            self.cb_counter[node.cb] += 1\n",
    "            \n",
    "        population = pd.read_csv(self.base + 'us2019_yolo.csv', dtype={'block_fips':str})\n",
    "        self.all_cb = population.block_fips.values.tolist()\n",
    "        self.all_cb.sort()\n",
    "        \n",
    "        #load sampling locations\n",
    "        self.locations = pd.read_csv(self.base + 'COD sampling MHs.csv')\n",
    "            \n",
    "    def locate_coor(self, coor):\n",
    "        '''\n",
    "        This functions reads a coordinate (format long, lat) and find the closest manhole\n",
    "        '''\n",
    "        if self.all_nodes.get(coor, None) is None: # not a real node\n",
    "            smallest = 10000\n",
    "            for coor0 in self.all_nodes.keys():\n",
    "                dist = (coor0[0] - coor[0])**2 + (coor0[1] - coor[1])**2\n",
    "                if dist < smallest:\n",
    "                    smallest = dist\n",
    "                    target_coor = coor0\n",
    "            coor = target_coor\n",
    "            return coor\n",
    "        else:\n",
    "            return coor\n",
    "    \n",
    "    def dfs(self, coor, visited, direction):\n",
    "        '''\n",
    "        This is a inside utility function. Do not use.\n",
    "        DFS starts from a manhole and look for its upstreams/downstreams, depending on the value of direction\n",
    "        '''\n",
    "        if not visited[coor]:\n",
    "            visited[coor] = 1\n",
    "            if direction == 'upstream':\n",
    "                next_list = self.all_nodes[coor].father\n",
    "            else:\n",
    "                next_list = self.all_nodes[coor].sons\n",
    "            for next_coor in next_list:\n",
    "                self.dfs(next_coor, visited, direction)\n",
    "                \n",
    "    def find_connection(self, coor, direction):\n",
    "        '''\n",
    "        Given a coordinate (long, lat), find its nearest manhole and search for its all upstreams/downstreams\n",
    "        '''\n",
    "        direction = direction.lower()\n",
    "        assert direction in ['downstream', 'upstream']\n",
    "        \n",
    "        coor = self.locate_coor(coor)\n",
    "        visited = defaultdict(int)\n",
    "        self.dfs(coor, visited, direction)\n",
    "        origin = coor\n",
    "        \n",
    "        all_x = []\n",
    "        all_y = []\n",
    "        for new_coor in visited.keys():\n",
    "            if visited[new_coor]:\n",
    "                all_x.append(new_coor[0])\n",
    "                all_y.append(new_coor[1])\n",
    "                \n",
    "        return all_x, all_y\n",
    "    \n",
    "    def plot_connection(self, coor, direction):\n",
    "        '''\n",
    "        Given a coordinate (long, lat), find its nearest manhole and plot its all upstreams/downstreams\n",
    "        '''\n",
    "    \n",
    "        direction = direction.lower()\n",
    "        assert direction in ['downstream', 'upstream']\n",
    "        \n",
    "        plt.figure(figsize=(17, 8))\n",
    "        \n",
    "        all_x = []\n",
    "        all_y = []\n",
    "        for key, value in self.coor_to_name.items():\n",
    "            all_x.append(key[0])\n",
    "            all_y.append(key[1])\n",
    "        \n",
    "        plt.scatter(all_x, all_y, label='normal')\n",
    "        \n",
    "        coor = self.locate_coor(coor)\n",
    "        all_x, all_y = self.find_connection(coor, direction)\n",
    "        \n",
    "        plt.scatter(all_x, all_y, label=direction)\n",
    "        \n",
    "        plt.scatter(coor[0], coor[1], label='source', color='red', marker='s')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.title(\"{} nodes for node {}\".format(direction, list(self.coor_to_name[coor])[0]))\n",
    "        plt.show()\n",
    "        \n",
    "    def get_population_composition(self, node_name):\n",
    "        '''\n",
    "        Inside utility function, do not use.\n",
    "        Get the population composition at a give manhole\n",
    "        '''\n",
    "        node_name += '-1'\n",
    "        node = self.all_nodes[self.name_to_coor[node_name]]\n",
    "        return node.population\n",
    "    \n",
    "    def process_HDT_data(self, path):\n",
    "        '''\n",
    "        Process HDT data (v3) so that it can be fed to `find_collection_points` method for further analysis\n",
    "        \n",
    "        Params:\n",
    "        =======\n",
    "        path: string, the directory for the target HDT file\n",
    "        '''\n",
    "        data = pd.read_csv(path, dtype={'CensusGEOID':str})\n",
    "        data = data[['ResultDate', 'Result', 'CensusGEOID']]\n",
    "        data.dropna(how='any', inplace=True)\n",
    "        data['CensusGEOID'] = data['CensusGEOID'].apply(lambda x : x if len(x) == 15 else '0' + x)\n",
    "        def get_date(x):\n",
    "            x = x.split(' ')[0]\n",
    "            m, d, y = x.split('/')\n",
    "            m = int(m)\n",
    "            d = int(d)\n",
    "            y = int(y)\n",
    "            date = datetime.date(y, m, d)\n",
    "            return date\n",
    "        \n",
    "        data['date'] = data['ResultDate'].apply(get_date)\n",
    "        data['Result'] = data['Result'].apply(lambda x : 1 if x == 'Detected' else 0)\n",
    "        data = data.groupby(['date', 'CensusGEOID']).aggregate('sum')\n",
    "        data.reset_index(drop=False, inplace=True)\n",
    "        data.rename(columns = {'CensusGEOID':'census_block', 'Result':'positive'}, inplace=True)\n",
    "        data = data[data['census_block'].apply(lambda x : x.startswith('06113'))]\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        return data\n",
    "    \n",
    "    def process_HDT_data_all_tests(self, path):\n",
    "        '''\n",
    "        Process HDT data (v3) so that it can be fed to `find_collection_points` method for further analysis\n",
    "        \n",
    "        Params:\n",
    "        =======\n",
    "        path: string, the directory for the target HDT file\n",
    "        '''\n",
    "        data = pd.read_csv(path, dtype={'CensusGEOID':str})\n",
    "        data = data[['ResultDate', 'Result', 'CensusGEOID']]\n",
    "        data.dropna(how='any', inplace=True)\n",
    "        data['CensusGEOID'] = data['CensusGEOID'].apply(lambda x : x if len(x) == 15 else '0' + x)\n",
    "        def get_date(x):\n",
    "            x = x.split(' ')[0]\n",
    "            m, d, y = x.split('/')\n",
    "            m = int(m)\n",
    "            d = int(d)\n",
    "            y = int(y)\n",
    "            date = datetime.date(y, m, d)\n",
    "            return date\n",
    "        \n",
    "        data['date'] = data['ResultDate'].apply(get_date)\n",
    "        data['Result'] = data['Result'].apply(lambda x : 1 if x == 'Detected' else 1)\n",
    "        data = data.groupby(['date', 'CensusGEOID']).aggregate('sum')\n",
    "        data.reset_index(drop=False, inplace=True)\n",
    "        data.rename(columns = {'CensusGEOID':'census_block', 'Result':'positive'}, inplace=True)\n",
    "        data = data[data['census_block'].apply(lambda x : x.startswith('06113'))]\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        return data\n",
    "        \n",
    "    def find_collection_points(self, source_locations):\n",
    "        '''\n",
    "        For a given DataFrame of Infection locations, find the expectation of infection at each collection point\n",
    "        \n",
    "        Params:\n",
    "        source_locations : pd.DataFrame, should have at least two columns.\n",
    "            column 'census_block', shows which census blocks have infections\n",
    "            column 'positive', shows positive counts for each corresponding census block\n",
    "            (optional) column 'date', indicating the date when the record is collected\n",
    "        '''\n",
    "        #check if there are multiple dates\n",
    "        if 'date' in source_locations.columns:\n",
    "            start_date = source_locations['date'].min()\n",
    "            end_date = source_locations['date'].max()\n",
    "            days = (end_date - start_date).days\n",
    "            result = pd.DataFrame({})\n",
    "            for i in tqdm.tqdm(range(days + 1)):\n",
    "                date = start_date + datetime.timedelta(days=i)\n",
    "                temp = source_locations[source_locations['date'] == date]\n",
    "                temp.reset_index(inplace=True, drop=True)\n",
    "                temp = temp[['census_block', 'positive']]\n",
    "                temp = self.find_collection_points(temp)\n",
    "                temp = temp[temp['total_infection'] > 0]\n",
    "                temp['date'] = date\n",
    "                result = result.append(temp)\n",
    "            result.reset_index(inplace=True, drop=True)\n",
    "            return result\n",
    "        \n",
    "        #check initial values\n",
    "        infection = defaultdict(lambda : 0)\n",
    "        for cb, count in zip(source_locations.census_block, source_locations.positive):\n",
    "            infection[cb] = count\n",
    "        \n",
    "        #topological sort\n",
    "        in_order = defaultdict(int)\n",
    "        for coor, node in self.all_nodes.items():\n",
    "            cb = node.cb\n",
    "            node.population = defaultdict(float)\n",
    "            node.population[cb] = infection[cb] / self.cb_counter[cb]\n",
    "            for next_coor in node.sons:\n",
    "                in_order[next_coor] += 1\n",
    "        \n",
    "        queue = []\n",
    "        for coor, node in self.all_nodes.items():\n",
    "            if in_order[coor] == 0:\n",
    "                queue.append(coor)\n",
    "                \n",
    "        while queue:\n",
    "            next_queue = []\n",
    "            for coor in queue:\n",
    "                node = self.all_nodes[coor]\n",
    "                N = len(node.sons)\n",
    "                for next_coor in node.sons:\n",
    "                    next_node = self.all_nodes[next_coor]\n",
    "                    for key, value in node.population.items():\n",
    "                        next_node.population[key] += value / len(node.sons)\n",
    "                    in_order[next_coor] -= 1\n",
    "                    if in_order[next_coor] == 0:\n",
    "                        next_queue.append(next_coor)\n",
    "            queue = next_queue\n",
    "        \n",
    "        infection = [infection[cb] for cb in self.all_cb]\n",
    "        MH_to_cb = {'census_block':self.all_cb, 'total_infection':infection}\n",
    "        for row in range(self.locations.shape[0]):\n",
    "            MH = self.locations.loc[row, 'MH ID']\n",
    "            composition = self.get_population_composition(MH)\n",
    "            temp_values = []\n",
    "            for cb in self.all_cb:\n",
    "                temp_values.append(composition[cb])\n",
    "            MH_to_cb[MH] = temp_values\n",
    "            \n",
    "        return pd.DataFrame(MH_to_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Graph from existing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = waste_water_processor('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of plotting Upstream/Downstream\n",
    "\n",
    "Use `graph.plot_connection` method, pass in a coordinate and the direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.plot_connection(graph.name_to_coor['M16-011-1'], 'upstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph.plot_connection(graph.name_to_coor['M16-011-1'], 'downstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for row in range(graph.locations.shape[0]):\n",
    "    long = graph.locations.loc[row, 'Long']\n",
    "    lat = graph.locations.loc[row, 'Lat']\n",
    "    graph.plot_connection((long, lat), 'upstream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composition of Waste Water Source\n",
    "\n",
    "Here we assume that each infected person produces the same amount of waste water each day. This amount is called **a unit**. We further assume that for each census block, all manholes in that census block has the same probability of collecting the waste water produced by that infected person. \n",
    "\n",
    "Here we check the **expect value of units of waste water produced by infected people at each collection manhoel**.\n",
    "\n",
    "## Example for a single date\n",
    "\n",
    "To check how waste water produced by infected people are collected, you should call `graph.find_collection_points` method and pass a `pd.DataFrame` object as input. The dataframe should have two columns, `'census_block'` and `'positive'`, recording the number of positive cases in each census block.\n",
    "\n",
    "**For examples for data with multiple dates, please check the end of this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of an input\n",
    "source = pd.DataFrame({'census_block':['061130106021000', '061130106021001', '061130106021008', '061130106021009'], \n",
    "                       'positive':[20, 10, 5, 8]})\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_points = graph.find_collection_points(source)\n",
    "collection_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see that some census blocks are not collected\n",
    "values = collection_points.to_numpy()[:, 1:]\n",
    "collection_points[values.sum(axis=1) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many units of waste water are collected at each collection point?\n",
    "\n",
    "Some collection points covers no population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population data\n",
    "population = pd.read_csv('./us2019_yolo.csv', dtype={'block_fips':str})\n",
    "population.rename(columns={'block_fips':'census_block', 'pop2019':'positive'}, inplace=True)\n",
    "population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = graph.find_collection_points(population)\n",
    "collection.drop(columns = ['census_block', 'total_infection'], inplace=True)\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.to_csv('collection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.plot_connection(graph.name_to_coor['N13-045-1'], 'upstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.plot_connection(graph.name_to_coor['O15-078-1'], 'upstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.plot_connection(graph.name_to_coor['P15-027-1'], 'upstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.plot_connection(graph.name_to_coor['O16-041-1'], 'upstream')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is waste water from each census block collected?\n",
    "\n",
    "We divide the census blocks into four types:\n",
    "  \n",
    "  * All waste water collected by one collection point\n",
    "  * All waste water collected, but by more than one collection points\n",
    "  * Not all waste water collected\n",
    "  * No waste water collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population['positive'] = 1\n",
    "weights = graph.find_collection_points(population)\n",
    "weights.set_index('census_block', inplace=True)\n",
    "weights.drop(columns = ['total_infection'], inplace=True)\n",
    "values = weights.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_collected = values.sum(axis=1)\n",
    "collected_by = (values > 0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type 1. All waste water collected by one collection point\n",
    "weights[np.logical_and(total_collected == 1, collected_by == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type 2. All waste water collected, but by more than one collection points\n",
    "weights[np.logical_and(total_collected >= 1, collected_by > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Type 3. Not all waste water collected\n",
    "weights[np.logical_and(total_collected < 1, 0 < total_collected)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type 4. No waste water collected\n",
    "weights[total_collected == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.to_csv('weights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for multiple dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = waste_water_processor('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to change the location to the HDT file here\n",
    "data = graph.process_HDT_data('./FILENAME.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = graph.find_collection_points(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.to_csv('OUTPUT_FILENAME.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_tests = graph.process_HDT_data_all_tests('./FILENAME.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_all_tests = graph.find_collection_points(data_all_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_all_tests.to_csv('OUTPUT_FILENAME.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
